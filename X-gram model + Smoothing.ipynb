{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 2 on Natural Language Processing\n",
    "\n",
    "### Date : 15th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stk58juYkzEr"
   },
   "source": [
    "**Dataset:** \n",
    "\n",
    " Use the text file provided along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT6byv49kdmo"
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "# read dataset\n",
    "f = open('C:\\\\Users\\\\Rushali\\\\NLP\\\\corpus.txt', encoding = 'utf8')\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRGqKaDn1pJy"
   },
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1OtHn6B1oc2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "words = text.split()\n",
    "s = sent_tokenize(text)\n",
    "s_new = []\n",
    "\n",
    "import string\n",
    "for sen in s:\n",
    "    new = sen.translate(str.maketrans('', '', string.punctuation))\n",
    "    new = ''.join(c for c in new if c not in '“' and c not in '”')\n",
    "    s_new.append(new.lower())\n",
    "s = s_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDL7yfpXkMRS"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3oIulBikPua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrams without add 1 smoothing and without stopword removal are : [('the', 1629), ('and', 842), ('to', 720), ('a', 626), ('she', 537), ('it', 522), ('of', 508), ('said', 458), ('i', 391), ('alice', 385)]\n",
      "\n",
      "Top 10 bigrams without add 1 smoothing and without stopword removal are : [(('said', 'the'), 208), (('of', 'the'), 130), (('said', 'alice'), 115), (('in', 'a'), 97), (('and', 'the'), 80), (('in', 'the'), 78), (('it', 'was'), 73), (('to', 'the'), 69), (('the', 'queen'), 65), (('as', 'she'), 61)]\n",
      "\n",
      "Top 10 trigrams without add 1 smoothing and without stopword removal are : [(('the', 'mock', 'turtle'), 51), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 20), (('said', 'to', 'herself'), 19), (('said', 'the', 'mock'), 19), (('said', 'the', 'caterpillar'), 18), (('she', 'went', 'on'), 17), (('she', 'said', 'to'), 17)]\n",
      "\n",
      "Top 10 fourgrams without add 1 smoothing and without stopword removal are : [(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('said', 'the', 'march', 'hare'), 8), (('will', 'you', 'won’t', 'you'), 8), (('said', 'alice', 'in', 'a'), 7), (('as', 'well', 'as', 'she'), 6), (('well', 'as', 'she', 'could'), 6), (('in', 'a', 'great', 'hurry'), 6), (('in', 'a', 'tone', 'of'), 6)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Write code\n",
    "\n",
    "from nltk.util import ngrams\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "fourgrams=[]\n",
    "\n",
    "\n",
    "for content in (s_new):\n",
    "    unigrams.extend(ngrams(content.split(),1))\n",
    "    bigrams.extend(ngrams(content.split(),2))\n",
    "    trigrams.extend(ngrams(content.split(),3))\n",
    "    fourgrams.extend(ngrams(content.split(),4))\n",
    "    ##similar for trigrams and fourgrams\n",
    "unigrams = [p for (p,) in unigrams]\n",
    "\n",
    "Fdist1 = nltk.FreqDist(unigrams)\n",
    "Fdist2 = nltk.FreqDist(bigrams)\n",
    "Fdist3 = nltk.FreqDist(trigrams)\n",
    "Fdist4 = nltk.FreqDist(fourgrams)\n",
    "\n",
    "print('Top 10 unigrams without add 1 smoothing and without stopword removal are : ' + str(Fdist1.most_common(10)) + '\\n' )\n",
    "print('Top 10 bigrams without add 1 smoothing and without stopword removal are : ' + str(Fdist2.most_common(10))+ '\\n')\n",
    "print('Top 10 trigrams without add 1 smoothing and without stopword removal are : ' + str(Fdist3.most_common(10))+ '\\n')\n",
    "print('Top 10 fourgrams without add 1 smoothing and without stopword removal are : ' + str(Fdist4.most_common(10))+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vARsvSfynePr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrams without add 1 smoothing and with stopword removal are : [('said', 458), ('alice', 385), ('little', 127), ('one', 101), ('like', 84), ('would', 83), ('went', 83), ('know', 81), ('could', 77), ('thought', 74)]\n",
      "\n",
      "Top 10 bigrams without add 1 smoothing and with stopword removal are : [(('said', 'the'), 208), (('said', 'alice'), 115), (('the', 'queen'), 65), (('the', 'king'), 60), (('a', 'little'), 59), (('mock', 'turtle'), 54), (('the', 'mock'), 53), (('the', 'gryphon'), 53), (('the', 'hatter'), 51), (('went', 'on'), 48)]\n",
      "\n",
      "Top 10 trigrams without add 1 smoothing and with stopword removal are : [(('the', 'mock', 'turtle'), 51), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 20), (('said', 'to', 'herself'), 19), (('said', 'the', 'mock'), 19), (('said', 'the', 'caterpillar'), 18), (('she', 'went', 'on'), 17), (('she', 'said', 'to'), 17)]\n",
      "\n",
      "Top 10 fourgrams without add 1 smoothing and with stopword removal are : [(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('said', 'the', 'march', 'hare'), 8), (('will', 'you', 'won’t', 'you'), 8), (('said', 'alice', 'in', 'a'), 7), (('as', 'well', 'as', 'she'), 6), (('well', 'as', 'she', 'could'), 6), (('in', 'a', 'great', 'hurry'), 6), (('in', 'a', 'tone', 'of'), 6)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "#print top 10 unigrams, bigrams after removing stopwords\n",
    "uni_processed = [p for p in unigrams if p not in stop]\n",
    "bi_processed = [(a,b) for (a,b) in bigrams if a not in stop or b not in stop]\n",
    "tri_processed = [(a,b,c) for (a,b,c) in trigrams if a not in stop or b not in stop or c not in stop]\n",
    "four_processed = [(a,b,c,d) for (a,b,c,d) in fourgrams if a not in stop or b not in stop or c not in stop or d not in stop]\n",
    "fdist1 = nltk.FreqDist(uni_processed)\n",
    "fdist2 = nltk.FreqDist(bi_processed)\n",
    "fdist3 = nltk.FreqDist(tri_processed)\n",
    "fdist4 = nltk.FreqDist(four_processed)\n",
    "\n",
    "#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n",
    "print('Top 10 unigrams without add 1 smoothing and with stopword removal are : ' + str(fdist1.most_common(10)) + '\\n' )\n",
    "print('Top 10 bigrams without add 1 smoothing and with stopword removal are : ' + str(fdist2.most_common(10)) + '\\n' )\n",
    "print('Top 10 trigrams without add 1 smoothing and with stopword removal are : ' + str(fdist3.most_common(10)) + '\\n' )\n",
    "print('Top 10 fourgrams without add 1 smoothing and with stopword removal are : ' + str(fdist4.most_common(10)) + '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioc1xNjmnim-"
   },
   "source": [
    "# Applying Smoothing\n",
    "\n",
    "\n",
    "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
    "\n",
    "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
    "\n",
    "N: Total number of N-grams <br>\n",
    "V: Number of unique N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grh4sO0Yns4V"
   },
   "outputs": [],
   "source": [
    "#You are to perform Add-1 smoothing here:\n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "import pandas as pd\n",
    "def newtext(textn):\n",
    "    unique_uni = []\n",
    "    unique_bi = []\n",
    "    unique_tri = []\n",
    "    unique_four = []\n",
    "    count_uni = []\n",
    "    count_bi = []\n",
    "    count_tri = []\n",
    "    count_four = []\n",
    "    V2 = 0\n",
    "    V3 = 0\n",
    "    V4 = 0\n",
    "    wordss = []\n",
    "    sentences = sent_tokenize(textn)\n",
    "    sentences_new = []\n",
    "    for sentence in sentences:\n",
    "        news = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        news = ''.join(c for c in new if c not in '“' and c not in '”')\n",
    "        sentences_new.append(new.lower())\n",
    "    sentences = sentences_new\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordss.append(word for word in sentence.split())\n",
    "        w = len(sentence.split())\n",
    "        V2 = V2 + (w-1)\n",
    "        V3 = V3 + (w-2)\n",
    "        V4 = V4 + (w-4)\n",
    "        \n",
    "    V1 = len(wordss)\n",
    "    N1 = len(unigrams)\n",
    "    N2 = len(bigrams)\n",
    "    N3 = len(trigrams)\n",
    "    N4 = len(fourgrams)\n",
    "    \n",
    "    for word in wordss:\n",
    "        unigrams.append(word)\n",
    "    for i in range(V1-1):\n",
    "        bigrams.append((wordss[i],wordss[i+1]))\n",
    "    for i in range(V1-2):\n",
    "        trigrams.append((wordss[i], wordss[i+1], wordss[i+2]))\n",
    "    for i in range(V1-3):\n",
    "        fourgrams.append((wordss[i],wordss[i+1], wordss[i+2], wordss[i+3]))\n",
    "    \n",
    "    for unigram in unigrams:\n",
    "        if unigram not in unique_uni:\n",
    "            unique_uni.append(unigram)\n",
    "    for (a,b) in bigrams:\n",
    "        if (a,b) not in unique_bi:\n",
    "            unique_bi.append((a,b))\n",
    "    for (a,b,c) in trigrams:\n",
    "        if (a,b,c) not in unique_tri:\n",
    "            unique_tri.append((a,b,c))\n",
    "    for (a,b,c,d) in fourgrams:\n",
    "        if (a,b,c,d) not in unique_four:\n",
    "            unique_four.append((a,b,c,d))\n",
    "\n",
    "    for unigram in unique_uni:\n",
    "        count_uni.append(unigrams.count(unigram))\n",
    "    for (a,b) in unique_bi:\n",
    "        count_bi.append(bigrams.count((a,b)))\n",
    "    for (a,b,c) in unique_tri:\n",
    "        count_tri.append(trigrams.count((a,b,c)))\n",
    "    for (a,b,c,d) in unique_four:\n",
    "        count_four.append(fourgrams.count((a,b,c,d)))\n",
    "        \n",
    "    df_uni = pd.DataFrame({'Unigrams' : unique_uni, 'Count' : count_uni, 'Probabilities' : [(count+1)/(N1+V1) for count in count_uni]})\n",
    "    df_bi = pd.DataFrame({'Bigrams' : unique_bi, 'Count' : count_bi, 'Probabilities' : [(count+1)/(N2+V2) for count in count_bi]})\n",
    "    df_tri = pd.DataFrame({'Trigrams' : unique_tri, 'Count' : count_tri, 'Probabilities' : [(count+1)/(N3+V3) for count in count_tri]})\n",
    "    df_four = pd.DataFrame({'Fourgrams' : unique_four, 'Count' : count_four, 'Probabilities' : [(count+1)/(N4+V4) for count in count_four]}) \n",
    "    #Print top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "    print('Top 10 unigrams after smoothing with are : ' + str(list(df_uni.nlargest(10, ['Probabilities'])['Unigrams'])) + '\\n')\n",
    "    print('Top 10 bigrams after smoothing are : ' + str(list(df_bi.nlargest(10, ['Probabilities'])['Bigrams'])) + '\\n')\n",
    "    print('Top 10 trigrams after smoothing are : ' + str(list(df_tri.nlargest(10, ['Probabilities'])['Trigrams'])) + '\\n')\n",
    "    print('Top 10 fourgrams after smoothing are : ' + str(list(df_four.nlargest(10, ['Probabilities'])['Fourgrams'])) + '\\n')\n",
    "    \n",
    "    return df_bi, df_tri, df_four\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GL40mQmmt4"
   },
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
    "\n",
    "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
    "For example, for the string 'He looked very' the answers can be as below: \n",
    ">     (1) 'He looked very' *anxiouxly* \n",
    ">     (2) 'He looked very' *uncomfortable* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWKo5_Fmnbg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrams after smoothing with are : ['the', 'and', 'to', 'a', 'she', 'it', 'of', 'said', 'i', 'alice']\n",
      "\n",
      "Top 10 bigrams after smoothing are : [('said', 'the'), ('of', 'the'), ('said', 'alice'), ('in', 'a'), ('and', 'the'), ('in', 'the'), ('it', 'was'), ('to', 'the'), ('the', 'queen'), ('as', 'she')]\n",
      "\n",
      "Top 10 trigrams after smoothing are : [('the', 'mock', 'turtle'), ('the', 'march', 'hare'), ('said', 'the', 'king'), ('the', 'white', 'rabbit'), ('said', 'the', 'hatter'), ('said', 'to', 'herself'), ('said', 'the', 'mock'), ('said', 'the', 'caterpillar'), ('she', 'went', 'on'), ('she', 'said', 'to')]\n",
      "\n",
      "Top 10 fourgrams after smoothing are : [('said', 'the', 'mock', 'turtle'), ('she', 'said', 'to', 'herself'), ('a', 'minute', 'or', 'two'), ('said', 'the', 'march', 'hare'), ('will', 'you', 'won’t', 'you'), ('said', 'alice', 'in', 'a'), ('as', 'well', 'as', 'she'), ('well', 'as', 'she', 'could'), ('in', 'a', 'great', 'hurry'), ('in', 'a', 'tone', 'of')]\n",
      "\n",
      "Top 10 unigrams after smoothing with are : ['the', 'and', 'to', 'a', 'she', 'it', 'of', 'said', 'i', 'alice']\n",
      "\n",
      "Top 10 bigrams after smoothing are : [('said', 'the'), ('of', 'the'), ('said', 'alice'), ('in', 'a'), ('and', 'the'), ('in', 'the'), ('it', 'was'), ('to', 'the'), ('the', 'queen'), ('as', 'she')]\n",
      "\n",
      "Top 10 trigrams after smoothing are : [('the', 'mock', 'turtle'), ('the', 'march', 'hare'), ('said', 'the', 'king'), ('the', 'white', 'rabbit'), ('said', 'the', 'hatter'), ('said', 'to', 'herself'), ('said', 'the', 'mock'), ('said', 'the', 'caterpillar'), ('she', 'went', 'on'), ('she', 'said', 'to')]\n",
      "\n",
      "Top 10 fourgrams after smoothing are : [('said', 'the', 'mock', 'turtle'), ('she', 'said', 'to', 'herself'), ('a', 'minute', 'or', 'two'), ('said', 'the', 'march', 'hare'), ('will', 'you', 'won’t', 'you'), ('said', 'alice', 'in', 'a'), ('as', 'well', 'as', 'she'), ('well', 'as', 'she', 'could'), ('in', 'a', 'great', 'hurry'), ('in', 'a', 'tone', 'of')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'\n",
    "df_bi1, df_tri1, df_four1 = newtext(str1)\n",
    "df_bi2, df_tri2, df_four2 = newtext(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ext_nVn2mvZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for str1 based on Bigrams only : \n",
      "after that alice said the *queen*\n",
      "after that alice said the *king*\n",
      "after that alice said the *mock*\n",
      "after that alice said the *gryphon*\n",
      "after that alice said the *hatter*\n",
      "\n",
      "Predictions for str1 based on Trigrams only : \n",
      "after that alice said the *king*\n",
      "after that alice said the *hatter*\n",
      "after that alice said the *mock*\n",
      "after that alice said the *caterpillar*\n",
      "after that alice said the *gryphon*\n",
      "\n",
      "Predictions for str1 based on Fourgrams only : \n",
      "NULL\n",
      "\n",
      "Predictions for str2 based on Bigrams only : \n",
      "alice felt so desperate that she was *a*\n",
      "alice felt so desperate that she was *the*\n",
      "alice felt so desperate that she was *not*\n",
      "alice felt so desperate that she was *going*\n",
      "alice felt so desperate that she was *that*\n",
      "\n",
      "Predictions for str2 based on Trigrams only : \n",
      "alice felt so desperate that she was *now*\n",
      "alice felt so desperate that she was *quite*\n",
      "alice felt so desperate that she was *a*\n",
      "alice felt so desperate that she was *walking*\n",
      "alice felt so desperate that she was *looking*\n",
      "\n",
      "Predictions for str2 based on Fourgrams only : \n",
      "alice felt so desperate that she was *now*\n",
      "alice felt so desperate that she was *dozing*\n",
      "alice felt so desperate that she was *walking*\n",
      "alice felt so desperate that she was *ready*\n",
      "alice felt so desperate that she was *in*\n"
     ]
    }
   ],
   "source": [
    "#write code\n",
    "wordsstr1 = str1.split()\n",
    "wordsstr2 = str2.split()\n",
    "prob1_bi = []\n",
    "n1_bi = []\n",
    "prob2_bi = []\n",
    "n2_bi = []\n",
    "prob1_tri = []\n",
    "n1_tri = []\n",
    "prob1_four = []\n",
    "n1_four = []\n",
    "prob2_tri = []\n",
    "n2_tri = []\n",
    "prob2_four = []\n",
    "n2_four = []\n",
    "\n",
    "print('Predictions for str1 based on Bigrams only : ')\n",
    "for i in range(len(df_bi1['Bigrams'])):\n",
    "    (a,b) = df_bi1.loc[i]['Bigrams']\n",
    "    if a == wordsstr1[-1]:\n",
    "        prob1_bi.append(df_bi1.loc[i]['Probabilities'])\n",
    "        n1_bi.append(b)\n",
    "dfnew1_bi = pd.DataFrame({'Bigrams' : n1_bi, 'Probabilities': prob1_bi})\n",
    "pred1_bi = list(dfnew1_bi.nlargest(5,'Probabilities')['Bigrams'])\n",
    "if not pred1_bi:\n",
    "    print('NULL')\n",
    "else:\n",
    "    for word in pred1_bi:\n",
    "        print(str1 + ' *' + word + '*')\n",
    "\n",
    "print('\\nPredictions for str1 based on Trigrams only : ')\n",
    "for i in range(len(df_tri1['Trigrams'])):\n",
    "    (a,b,c) = df_tri1.loc[i]['Trigrams']\n",
    "    if a == wordsstr1[-2] and b == wordsstr1[-1]:\n",
    "        prob1_tri.append(df_tri1.loc[i]['Probabilities'])\n",
    "        n1_tri.append(c)\n",
    "dfnew1_tri = pd.DataFrame({'Trigrams' : n1_tri, 'Probabilities': prob1_tri})\n",
    "pred1_tri = list(dfnew1_tri.nlargest(5,'Probabilities')['Trigrams'])\n",
    "if not pred1_tri:\n",
    "    print('NULL')\n",
    "else:\n",
    "    for word in pred1_tri:\n",
    "        print(str1 + ' *' + word + '*')\n",
    "    \n",
    "print('\\nPredictions for str1 based on Fourgrams only : ')\n",
    "for i in range(len(df_four1['Fourgrams'])):\n",
    "    (a,b,c,d) = df_four1.loc[i]['Fourgrams']\n",
    "    if a == wordsstr1[-3] and b == wordsstr1[-2] and c == wordsstr1[-1]:\n",
    "        prob1_four.append(df_four1.loc[i]['Probabilities'])\n",
    "        n1_four.append(d)\n",
    "dfnew1_four = pd.DataFrame({'Fourgrams' : n1_four, 'Probabilities': prob1_four})\n",
    "pred1_four = list(dfnew1_four.nlargest(5,'Probabilities')['Fourgrams'])\n",
    "if not pred1_four:\n",
    "    print('NULL')\n",
    "else:\n",
    "    for word in pred1_four:\n",
    "        print(str1 + ' *' + word + '*')\n",
    "\n",
    "print('\\nPredictions for str2 based on Bigrams only : ')\n",
    "for i in range(len(df_bi2['Bigrams'])):\n",
    "    (a,b) = df_bi2.loc[i]['Bigrams']\n",
    "    if a == wordsstr2[-1]:\n",
    "        prob2_bi.append(df_bi2.loc[i]['Probabilities'])\n",
    "        n2_bi.append(b)\n",
    "dfnew2_bi = pd.DataFrame({'Bigrams' : n2_bi, 'Probabilities': prob2_bi})\n",
    "pred2_bi = list(dfnew2_bi.nlargest(5,'Probabilities')['Bigrams'])\n",
    "if not pred2_bi:\n",
    "    print('NULL')\n",
    "else:\n",
    "    for word in pred2_bi:\n",
    "        print(str2 + ' *' + word + '*')\n",
    "\n",
    "print('\\nPredictions for str2 based on Trigrams only : ')\n",
    "for i in range(len(df_tri2['Trigrams'])):\n",
    "    (a,b,c) = df_tri2.loc[i]['Trigrams']\n",
    "    if a == wordsstr2[-2] and b == wordsstr2[-1]:\n",
    "        prob2_tri.append(df_tri2.loc[i]['Probabilities'])\n",
    "        n2_tri.append(c)\n",
    "dfnew2_tri = pd.DataFrame({'Trigrams' : n2_tri, 'Probabilities': prob2_tri})\n",
    "pred2_tri = list(dfnew2_tri.nlargest(5,'Probabilities')['Trigrams'])\n",
    "if not pred2_tri:\n",
    "    print('NULL')\n",
    "else:\n",
    "    for word in pred2_tri:\n",
    "        print(str2 + ' *' + word + '*')\n",
    "\n",
    "print('\\nPredictions for str2 based on Fourgrams only : ')\n",
    "for i in range(len(df_four2['Fourgrams'])):\n",
    "    (a,b,c,d) = df_four2.loc[i]['Fourgrams']\n",
    "    if a == wordsstr2[-3] and b == wordsstr2[-2] and c == wordsstr2[-1]:\n",
    "        prob2_four.append(df_four2.loc[i]['Probabilities'])\n",
    "        n2_four.append(d)\n",
    "dfnew2_four = pd.DataFrame({'Fourgrams' : n2_four, 'Probabilities': prob2_four})\n",
    "pred2_four = list(dfnew2_four.nlargest(5,'Probabilities')['Fourgrams'])\n",
    "if not pred2_four:\n",
    "    print('NULL')\n",
    "else:\n",
    "    for word in pred2_four:\n",
    "        print(str2 + ' *' + word + '*')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
